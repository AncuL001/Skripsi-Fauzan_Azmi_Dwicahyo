{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.06s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import cv2\n",
    "\n",
    "from taco_dataset import CoCoDatasetForYOLO\n",
    "from config import Config\n",
    "\n",
    "cfg = Config()\n",
    "\n",
    "train_transforms = A.Compose(\n",
    "    [\n",
    "        A.LongestMaxSize(max_size=int(cfg.IMAGE_SIZE * cfg.scale)),\n",
    "        A.PadIfNeeded(\n",
    "            min_height=int(cfg.IMAGE_SIZE * cfg.scale),\n",
    "            min_width=int(cfg.IMAGE_SIZE * cfg.scale),\n",
    "            border_mode=cv2.BORDER_CONSTANT,\n",
    "        ),\n",
    "        A.RandomCrop(width=cfg.IMAGE_SIZE, height=cfg.IMAGE_SIZE),\n",
    "        A.ColorJitter(brightness=0.6, contrast=0.6, saturation=0.6, hue=0.6, p=0.4),\n",
    "        A.OneOf(\n",
    "            [\n",
    "                A.ShiftScaleRotate(\n",
    "                    rotate_limit=20, p=0.5, border_mode=cv2.BORDER_CONSTANT\n",
    "                ),\n",
    "                A.Affine(shear=15, p=0.5, mode=cv2.BORDER_CONSTANT),\n",
    "            ],\n",
    "            p=1.0,\n",
    "        ),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.Blur(p=0.1),\n",
    "        A.CLAHE(p=0.1),\n",
    "        A.Posterize(p=0.1),\n",
    "        A.ToGray(p=0.1),\n",
    "        A.ChannelShuffle(p=0.05),\n",
    "        A.Normalize(mean=[0, 0, 0], std=[1, 1, 1], max_pixel_value=255,),\n",
    "        ToTensorV2(),\n",
    "    ],\n",
    "\n",
    "    # got this error\n",
    "    # Expected y_min for bbox (0.5355392156862745, -0.00015318627450980338, 0.6523692810457516, 0.1803002450980392, 0) to be in the range [0.0, 1.0], got -0.00015318627450980338.\n",
    "    # rounding issue :/\n",
    "    # the insane solution to this problem (modifying the library code) https://github.com/albumentations-team/albumentations/issues/459#issuecomment-734454278\n",
    "    bbox_params=A.BboxParams(format=\"yolo\", min_visibility=0.4, label_fields=[],),\n",
    ")\n",
    "\n",
    "train_dataset = CoCoDatasetForYOLO(\n",
    "    root=cfg.DATASET_PATH,\n",
    "    annFile=cfg.anns_file_path,\n",
    "    transform=train_transforms,\n",
    "    S=cfg.SPLIT_SIZE, B=cfg.NUM_BOXES, C=cfg.NUM_CLASSES\n",
    ")\n",
    "\n",
    "train_percentage = 0.8\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 885,  757,  734,  ..., 1001, 1481,  657])\n",
      "1200\n",
      "\n",
      "tensor([1110,  843,  459,  859, 1441,  610,  367, 1158,  914,  753, 1274, 1100,\n",
      "        1136, 1004,  190,  678,  195,  428,  477,  575,  191,  426,  158,  518,\n",
      "          50,  670, 1146, 1102,  910, 1485,  235,  541, 1337,  975,  967,  996,\n",
      "        1360,   22, 1458, 1488,  144,   36,  916,  635,  258, 1256,   69,  789,\n",
      "         504,   76, 1223,    5,  475, 1478,  441, 1265, 1436,  708,  418,  581,\n",
      "         314, 1065,  177, 1350,  765,  693,  147, 1096,  481, 1277, 1407,  570,\n",
      "         291,  207, 1371,  983,  345,  107, 1036, 1127,  860,  677,  183, 1116,\n",
      "         528, 1206, 1200,  126,  936,   90, 1069,  883,  278,  168,  811,  222,\n",
      "        1428, 1203,  440,  768,  869, 1252,  170,  468,   10,  707, 1020,  982,\n",
      "        1406, 1182,  795, 1193, 1258, 1013, 1320,  301, 1139,  752,  306,  928,\n",
      "         239,  663, 1168,  648, 1291,  866,  961, 1064,  870, 1317,   92,  938,\n",
      "        1474, 1394,  265, 1166,  445,  288,  321,  508,  727,  895,  948,   83,\n",
      "         837, 1227,  935,  780,    2,  280,  791,  223,   37,  977, 1154,  270,\n",
      "         136,  442,  825,   87,  298,  636,  154,  392, 1342,  368, 1285, 1338,\n",
      "        1424,   75,  599, 1453,  160, 1358,    4,   82,  384,  150, 1440, 1304,\n",
      "         628,  538,  845, 1409, 1121,  283,  756,  452,  654, 1117, 1380,  820,\n",
      "        1105, 1377, 1058,  871, 1254, 1482, 1319,  876, 1454,  470,   28,  563,\n",
      "         988, 1489,  593,  731,  162,  268,  281, 1202,  847, 1460,  830,  276,\n",
      "         902, 1416,  741,  146,  815,  976,  238, 1103,  188, 1287,  379, 1059,\n",
      "         614, 1126, 1115, 1067, 1323, 1063, 1307,  863,  793,   98,  218, 1298,\n",
      "         342,  521, 1250,  373,  784,  139,  120, 1292,  759,  474,   72, 1078,\n",
      "         644,  812,  220,  684,  826,  713,  361,  271, 1075, 1321,  193,  755,\n",
      "         750,  945, 1392,  334,  242, 1002,  742,  267,  639,  808, 1179,  471,\n",
      "         200, 1282,  420, 1495,   49, 1472, 1134,  176,  696, 1467, 1197,  632,\n",
      "         122,   80,  443,  923,  256,  493,  799,  491, 1185, 1062, 1215, 1039])\n",
      "300\n"
     ]
    }
   ],
   "source": [
    "indices = torch.randperm(len(train_dataset))\n",
    "test_size = round(len(train_dataset) * (1 - train_percentage))\n",
    "\n",
    "print(indices[:-test_size])\n",
    "print(len(indices[:-test_size]))\n",
    "print()\n",
    "print(indices[-test_size:])\n",
    "print(len(indices[-test_size:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1120,  839,  819,  ...,  546,  443,  480])\n",
      "1199\n",
      "\n",
      "tensor([1119,  817,  818, 1142,  870, 1096, 1103,  608, 1153,   84,  795,  408,\n",
      "          69, 1253,  792,  793, 1199, 1307,  308,  709, 1073,   24, 1039,  712,\n",
      "        1088, 1365, 1427,  156,  166,  862, 1191, 1247, 1135,  713,  410,  223,\n",
      "        1190, 1430,  469,  224,   65,  135,  312,  428,  600,  483,  132,   50,\n",
      "         614,  197,  331,  383, 1036,  262,  669, 1186, 1038,  432,  263, 1368,\n",
      "        1050, 1465, 1242, 1330,  666,  365,  535,   36,  220,  566,  875,  228,\n",
      "         730,  122,  523,  161, 1463,  849,  461,  957,  389,  830, 1321,  708,\n",
      "         610,  260,  863, 1206,  169,  214,  729, 1363,  683,  121,  366,  314,\n",
      "        1320,  107, 1294,  892, 1127,  941,  943,  991,  964,  939,  981,  778,\n",
      "         788,  336, 1010,  886,  973,  907,  947, 1332,  915,  972,  133, 1003,\n",
      "         636,  894,  978,  921, 1335,  893,  970,  984, 1196,  932,  899,  237,\n",
      "          98,   99,  271, 1383,  477,  745,  268,  602,  173, 1110,  784,  785,\n",
      "          87,  641,   10,   39, 1234,  238,  273,  202, 1489, 1337,  279,  776,\n",
      "         786, 1285,  667,  747,  538,  673,   41, 1276,  438, 1178,   77,  270,\n",
      "        1081, 1021, 1013,  138,  379, 1438,  541, 1157,  490,  373, 1472,  171,\n",
      "         117,  179,  677,  568,  579,  725,  774,  766, 1269,  858,  215,  203,\n",
      "         205, 1209,  676,  658,  557,  191,  374, 1428,  453,  385,  550,  380,\n",
      "         251, 1484, 1317, 1456, 1043,  225,  152,  494, 1302,  196,  808, 1102,\n",
      "         391, 1099,  416,  295, 1394,  601, 1055,  378, 1222,  126,  551,  485,\n",
      "        1195,   59,  150, 1211, 1049,  971,  247,  434,  242,  315,  953, 1059,\n",
      "         543, 1345,   95, 1080, 1229, 1274,  809, 1291, 1342, 1079,  881,   49,\n",
      "         344, 1447,  495, 1343, 1358,  442, 1314,  294,  187,  707, 1147, 1024,\n",
      "         395,  454,  458, 1237,   61, 1275,  755, 1300,  650,  758, 1248,   76,\n",
      "         625, 1098, 1026,  349,  396,   91,  298, 1305,   19,  272, 1058, 1386,\n",
      "         103, 1273,  493, 1215,  147,  259,  176,  448,  327,  587, 1233, 1471,\n",
      "         724])\n",
      "301\n"
     ]
    }
   ],
   "source": [
    "from utils import get_stratified_indices\n",
    "\n",
    "train_indices, test_indices = get_stratified_indices(cfg.anns_file_path, len(train_dataset), train_percentage)\n",
    "\n",
    "print(train_indices)\n",
    "print(len(train_indices))\n",
    "print()\n",
    "print(test_indices)\n",
    "print(len(test_indices))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
